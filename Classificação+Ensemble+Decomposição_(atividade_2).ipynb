{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "86169f25",
      "metadata": {
        "id": "86169f25"
      },
      "source": [
        "# Data Set -> Car Evaluation\n",
        "\n",
        "Esse dataset é composto de 6 variáveis categóricas: compra, manutenção, portas, capacidade de pessoas, bagageiro e segurança.\n",
        "\n",
        "A variável-alvo que indica a aceitação do carro:\n",
        "\n",
        "unacc = Não aceitável\n",
        "\n",
        "acc = Aceitável\n",
        "\n",
        "good = Bom!\n",
        "\n",
        "vgood = Muito Bom!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p1rq2GawNq0W",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "p1rq2GawNq0W",
        "outputId": "f975c219-0e7d-4e95-a485-dfd935234ac1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.12/dist-packages (0.0.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2025.8.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Dependências\n",
        "#!pip install ucimlrepo scikit-learn pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "m7MPiwlQNzT-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7MPiwlQNzT-",
        "outputId": "6d89d29d-4533-4c1e-808b-4e106385d19c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'uci_id': 19, 'name': 'Car Evaluation', 'repository_url': 'https://archive.ics.uci.edu/dataset/19/car+evaluation', 'data_url': 'https://archive.ics.uci.edu/static/public/19/data.csv', 'abstract': 'Derived from simple hierarchical decision model, this database may be useful for testing constructive induction and structure discovery methods.', 'area': 'Other', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 1728, 'num_features': 6, 'feature_types': ['Categorical'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1988, 'last_updated': 'Thu Aug 10 2023', 'dataset_doi': '10.24432/C5JP48', 'creators': ['Marko Bohanec'], 'intro_paper': {'ID': 249, 'type': 'NATIVE', 'title': 'Knowledge acquisition and explanation for multi-attribute decision making', 'authors': 'M. Bohanec, V. Rajkovič', 'venue': '8th Intl Workshop on Expert Systems and their Applications, Avignon, France', 'year': 1988, 'journal': None, 'DOI': None, 'URL': 'https://www.semanticscholar.org/paper/KNOWLEDGE-ACQUISITION-AND-EXPLANATION-FOR-DECISION-Bohanec-Rajkovi%C4%8D/8bab443ae322ff47c3e609272bd93fd4650555bc', 'sha': None, 'corpus': None, 'arxiv': None, 'mag': None, 'acl': None, 'pmid': None, 'pmcid': None}, 'additional_info': {'summary': 'Car Evaluation Database was derived from a simple hierarchical decision model originally developed for the demonstration of DEX, M. Bohanec, V. Rajkovic: Expert system for decision making. Sistemica 1(1), pp. 145-157, 1990.). The model evaluates cars according to the following concept structure:\\r\\n\\r\\nCAR                      car acceptability\\r\\n. PRICE                  overall price\\r\\n. . buying               buying price\\r\\n. . maint                price of the maintenance\\r\\n. TECH                   technical characteristics\\r\\n. . COMFORT              comfort\\r\\n. . . doors              number of doors\\r\\n. . . persons            capacity in terms of persons to carry\\r\\n. . . lug_boot           the size of luggage boot\\r\\n. . safety               estimated safety of the car\\r\\n\\r\\nInput attributes are printed in lowercase. Besides the target concept (CAR), the model includes three intermediate concepts: PRICE, TECH, COMFORT. Every concept is in the original model related to its lower level descendants by a set of examples (for these examples sets see http://www-ai.ijs.si/BlazZupan/car.html).\\r\\n\\r\\nThe Car Evaluation Database contains examples with the structural information removed, i.e., directly relates CAR to the six input attributes: buying, maint, doors, persons, lug_boot, safety.\\r\\n\\r\\nBecause of known underlying concept structure, this database may be particularly useful for testing constructive induction and structure discovery methods.\\r\\n', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'buying:   vhigh, high, med, low.\\nmaint:    vhigh, high, med, low.\\ndoors:    2, 3, 4, 5more.\\npersons:  2, 4, more.\\nlug_boot: small, med, big.\\nsafety:   low, med, high.', 'citation': None}}\n",
            "       name     role         type demographic  \\\n",
            "0    buying  Feature  Categorical        None   \n",
            "1     maint  Feature  Categorical        None   \n",
            "2     doors  Feature  Categorical        None   \n",
            "3   persons  Feature  Categorical        None   \n",
            "4  lug_boot  Feature  Categorical        None   \n",
            "5    safety  Feature  Categorical        None   \n",
            "6     class   Target  Categorical        None   \n",
            "\n",
            "                                         description units missing_values  \n",
            "0                                       buying price  None             no  \n",
            "1                           price of the maintenance  None             no  \n",
            "2                                    number of doors  None             no  \n",
            "3              capacity in terms of persons to carry  None             no  \n",
            "4                           the size of luggage boot  None             no  \n",
            "5                        estimated safety of the car  None             no  \n",
            "6  evaulation level (unacceptable, acceptable, go...  None             no  \n"
          ]
        }
      ],
      "source": [
        "# importação do dataset\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "# fetch dataset\n",
        "car_evaluation = fetch_ucirepo(id=19)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X = car_evaluation.data.features\n",
        "y = car_evaluation.data.targets\n",
        "\n",
        "# metadata\n",
        "print(car_evaluation.metadata)\n",
        "\n",
        "# variable information\n",
        "print(car_evaluation.variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "16559090",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16559090",
        "outputId": "6d58d3e1-5874-44b9-e238-3919f19aecb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "     buying  maint  doors persons lug_boot safety\n",
            "0     vhigh  vhigh      2       2    small    low\n",
            "1     vhigh  vhigh      2       2    small    med\n",
            "2     vhigh  vhigh      2       2    small   high\n",
            "3     vhigh  vhigh      2       2      med    low\n",
            "4     vhigh  vhigh      2       2      med    med\n",
            "...     ...    ...    ...     ...      ...    ...\n",
            "1723    low    low  5more    more      med    med\n",
            "1724    low    low  5more    more      med   high\n",
            "1725    low    low  5more    more      big    low\n",
            "1726    low    low  5more    more      big    med\n",
            "1727    low    low  5more    more      big   high\n",
            "\n",
            "[1728 rows x 6 columns]\n"
          ]
        }
      ],
      "source": [
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "676c24ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "676c24ae",
        "outputId": "210a47ec-fe72-49a9-e96d-72ec47217333"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      class\n",
            "0     unacc\n",
            "1     unacc\n",
            "2     unacc\n",
            "3     unacc\n",
            "4     unacc\n",
            "...     ...\n",
            "1723   good\n",
            "1724  vgood\n",
            "1725  unacc\n",
            "1726   good\n",
            "1727  vgood\n",
            "\n",
            "[1728 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cH0vvO-rhxRC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "cH0vvO-rhxRC",
        "outputId": "3664e358-d097-47e9-e90a-6f224e67b691"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "class\n",
              "unacc    1210\n",
              "acc       384\n",
              "good       69\n",
              "vgood      65\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.value_counts()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4vD48Mb6S-8V",
      "metadata": {
        "id": "4vD48Mb6S-8V"
      },
      "source": [
        "# 1º PIPELINE\n",
        "\n",
        "* Carrega o dataset Car Evaluation\n",
        "\n",
        "* Faz one-hot encoding das variáveis categóricas\n",
        "\n",
        "* Treina e avalia, com validação cruzada estratificada, os quatro modelos:\n",
        "\n",
        "  1.   Árvore de Decisão\n",
        "  2.   Bagging\n",
        "  3.   AdaBoost (stumps por padrão)\n",
        "  4.   Random Forest\n",
        "\n",
        "* Reporta accuracy e F1 macro (métrica mais robusta a desbalanceamento)\n",
        "\n",
        "* Separa um conjunto de teste para uma comparação final + matriz de confusão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1rscyfh4TByF",
      "metadata": {
        "id": "1rscyfh4TByF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Using cached scikit_learn-1.7.2-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\pichau\\desktop\\faculdade\\mestrado\\mineração de dados\\atividade 2\\.venv\\lib\\site-packages (from scikit-learn) (2.3.3)\n",
            "Collecting scipy>=1.8.0 (from scikit-learn)\n",
            "  Using cached scipy-1.16.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
            "Collecting joblib>=1.2.0 (from scikit-learn)\n",
            "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Using cached scikit_learn-1.7.2-cp313-cp313-win_amd64.whl (8.7 MB)\n",
            "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Using cached scipy-1.16.2-cp313-cp313-win_amd64.whl (38.5 MB)\n",
            "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
            "Successfully installed joblib-1.5.2 scikit-learn-1.7.2 scipy-1.16.2 threadpoolctl-3.6.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "%pip install scikit-learn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "RANDOM_STATE = 42\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "yVoy6--JTvNV",
      "metadata": {
        "id": "yVoy6--JTvNV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Dados\n",
        "# ---------------------------------------------------------------------\n",
        "car_evaluation = fetch_ucirepo(id=19)\n",
        "X = car_evaluation.data.features.copy()\n",
        "y = car_evaluation.data.targets.iloc[:, 0].copy()  # target vem como DF; pega a primeira coluna\n",
        "\n",
        "# Todas as colunas de X são categóricas nesse dataset\n",
        "categorical_cols = list(X.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "foZvjD4_Tz4V",
      "metadata": {
        "id": "foZvjD4_Tz4V"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 2) Pré-processamento: OneHot para categóricas\n",
        "# ---------------------------------------------------------------------\n",
        "ohe = OneHotEncoder(handle_unknown=\"ignore\")  # robusto a categorias raras no teste\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[(\"cat\", ohe, categorical_cols)],\n",
        "    remainder=\"drop\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "gfKx02jRT1_U",
      "metadata": {
        "id": "gfKx02jRT1_U"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 3) Modelos\n",
        "#    - Árvores com class_weight=\"balanced\" por causa do desbalanceamento das classes\n",
        "# ---------------------------------------------------------------------\n",
        "tree_clf = DecisionTreeClassifier(\n",
        "    criterion=\"gini\",\n",
        "    max_depth=None,\n",
        "    min_samples_leaf=1,\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(\n",
        "        criterion=\"gini\",\n",
        "        max_depth=None,\n",
        "        min_samples_leaf=1,\n",
        "        class_weight=\"balanced\",\n",
        "        random_state=RANDOM_STATE\n",
        "    ),\n",
        "    n_estimators=200,\n",
        "    max_samples=1.0,\n",
        "    max_features=1.0,\n",
        "    bootstrap=True,\n",
        "    n_jobs=-1,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "adaboost_clf = AdaBoostClassifier(\n",
        "    # Por padrão o AdaBoost usa stumps (árvores de profundidade 1).\n",
        "    # Você pode explicitar se quiser: estimator=DecisionTreeClassifier(max_depth=1, random_state=RANDOM_STATE)\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.5,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "rf_clf = RandomForestClassifier(\n",
        "    n_estimators=400,\n",
        "    max_depth=None,\n",
        "    min_samples_leaf=1,\n",
        "    class_weight=\"balanced\",\n",
        "    n_jobs=-1,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "models = {\n",
        "    \"DecisionTree\": tree_clf,\n",
        "    \"Bagging(Tree)\": bagging_clf,\n",
        "    \"AdaBoost\": adaboost_clf,\n",
        "    \"RandomForest\": rf_clf\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "me9iyhH4T5Gl",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "me9iyhH4T5Gl",
        "outputId": "8a883460-a7d3-42dd-b591-2d638e0d3666"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\PICHAU\\Desktop\\Faculdade\\Mestrado\\Mineração de Dados\\Atividade 2\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\PICHAU\\Desktop\\Faculdade\\Mestrado\\Mineração de Dados\\Atividade 2\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
            "c:\\Users\\PICHAU\\Desktop\\Faculdade\\Mestrado\\Mineração de Dados\\Atividade 2\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 4) Validação cruzada CV (k-fold estratificado) + holdout final\n",
        "# ---------------------------------------------------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "scoring = {\n",
        "    \"accuracy\": \"accuracy\",\n",
        "    \"f1_macro\": \"f1_macro\"\n",
        "}\n",
        "\n",
        "def summarize_scores(name, cv_results):\n",
        "    acc = cv_results[\"test_accuracy\"]\n",
        "    f1m = cv_results[\"test_f1_macro\"]\n",
        "    return {\n",
        "        \"model\": name,\n",
        "        \"cv_acc_mean\": acc.mean(),\n",
        "        \"cv_acc_std\": acc.std(ddof=1),\n",
        "        \"cv_f1m_mean\": f1m.mean(),\n",
        "        \"cv_f1m_std\": f1m.std(ddof=1),\n",
        "    }\n",
        "\n",
        "summary_rows = []\n",
        "final_reports = {}\n",
        "\n",
        "for name, clf in models.items():\n",
        "    pipe = Pipeline(steps=[\n",
        "        (\"prep\", preprocess),\n",
        "        (\"clf\", clf)\n",
        "    ])\n",
        "\n",
        "    # Validação cruzada no conjunto de treino\n",
        "    cv_results = cross_validate(\n",
        "        pipe, X_train, y_train,\n",
        "        scoring=scoring,\n",
        "        cv=cv,\n",
        "        n_jobs=-1,\n",
        "        return_estimator=False\n",
        "    )\n",
        "    summary_rows.append(summarize_scores(name, cv_results))\n",
        "\n",
        "    # Ajuste final e avaliação no holdout\n",
        "    pipe.fit(X_train, y_train)\n",
        "    y_pred = pipe.predict(X_test)\n",
        "\n",
        "    final_reports[name] = {\n",
        "        \"classification_report\": classification_report(y_test, y_pred, digits=4),\n",
        "        \"confusion_matrix\": confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "Tw5fAdeST7yF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw5fAdeST7yF",
        "outputId": "d466f04b-ec1e-4710-bbc3-9d8e560c3327"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================== RESUMO CV (5-fold) ====================\n",
            "        model  cv_acc_mean  cv_acc_std  cv_f1m_mean  cv_f1m_std\n",
            "Bagging(Tree)       0.9718      0.0119       0.9495      0.0136\n",
            " DecisionTree       0.9559      0.0113       0.9347      0.0160\n",
            " RandomForest       0.9522      0.0101       0.8830      0.0239\n",
            "     AdaBoost       0.8502      0.0170       0.4803      0.0520\n",
            "\n",
            "==================== TESTE (HOLDOUT 20%) ====================\n",
            "\n",
            ">>> DecisionTree\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         acc     0.8961    0.8961    0.8961        77\n",
            "        good     0.8571    0.8571    0.8571        14\n",
            "       unacc     0.9754    0.9835    0.9794       242\n",
            "       vgood     1.0000    0.8462    0.9167        13\n",
            "\n",
            "    accuracy                         0.9538       346\n",
            "   macro avg     0.9322    0.8957    0.9123       346\n",
            "weighted avg     0.9539    0.9538    0.9536       346\n",
            "\n",
            "Matriz de confusão (linhas = verdade, colunas = predito):\n",
            "[[ 69   2   6   0]\n",
            " [  2  12   0   0]\n",
            " [  4   0 238   0]\n",
            " [  2   0   0  11]]\n",
            "\n",
            ">>> Bagging(Tree)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         acc     0.9733    0.9481    0.9605        77\n",
            "        good     0.8750    1.0000    0.9333        14\n",
            "       unacc     0.9917    0.9917    0.9917       242\n",
            "       vgood     1.0000    1.0000    1.0000        13\n",
            "\n",
            "    accuracy                         0.9827       346\n",
            "   macro avg     0.9600    0.9849    0.9714       346\n",
            "weighted avg     0.9832    0.9827    0.9827       346\n",
            "\n",
            "Matriz de confusão (linhas = verdade, colunas = predito):\n",
            "[[ 73   2   2   0]\n",
            " [  0  14   0   0]\n",
            " [  2   0 240   0]\n",
            " [  0   0   0  13]]\n",
            "\n",
            ">>> AdaBoost\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         acc     0.6517    0.7532    0.6988        77\n",
            "        good     0.5455    0.4286    0.4800        14\n",
            "       unacc     0.9350    0.9504    0.9426       242\n",
            "       vgood     0.0000    0.0000    0.0000        13\n",
            "\n",
            "    accuracy                         0.8497       346\n",
            "   macro avg     0.5330    0.5331    0.5304       346\n",
            "weighted avg     0.8210    0.8497    0.8342       346\n",
            "\n",
            "Matriz de confusão (linhas = verdade, colunas = predito):\n",
            "[[ 58   3  16   0]\n",
            " [  8   6   0   0]\n",
            " [ 12   0 230   0]\n",
            " [ 11   2   0   0]]\n",
            "\n",
            ">>> RandomForest\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         acc     0.9259    0.9740    0.9494        77\n",
            "        good     1.0000    0.9286    0.9630        14\n",
            "       unacc     0.9917    0.9835    0.9876       242\n",
            "       vgood     1.0000    0.9231    0.9600        13\n",
            "\n",
            "    accuracy                         0.9769       346\n",
            "   macro avg     0.9794    0.9523    0.9650       346\n",
            "weighted avg     0.9777    0.9769    0.9770       346\n",
            "\n",
            "Matriz de confusão (linhas = verdade, colunas = predito):\n",
            "[[ 75   0   2   0]\n",
            " [  1  13   0   0]\n",
            " [  4   0 238   0]\n",
            " [  1   0   0  12]]\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# 5) Resultados\n",
        "# ---------------------------------------------------------------------\n",
        "summary_df = pd.DataFrame(summary_rows).sort_values(\"cv_f1m_mean\", ascending=False)\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "print(\"\\n==================== RESUMO CV (5-fold) ====================\")\n",
        "print(summary_df.to_string(index=False, float_format=lambda x: f\"{x:0.4f}\"))\n",
        "\n",
        "print(\"\\n==================== TESTE (HOLDOUT 20%) ====================\")\n",
        "for name in models.keys():\n",
        "    print(f\"\\n>>> {name}\")\n",
        "    print(final_reports[name][\"classification_report\"])\n",
        "    print(\"Matriz de confusão (linhas = verdade, colunas = predito):\")\n",
        "    print(final_reports[name][\"confusion_matrix\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AnKK2G2RVL_s",
      "metadata": {
        "id": "AnKK2G2RVL_s"
      },
      "source": [
        "# 2º PIPELINE\n",
        "\n",
        "* Carrega o dataset Car Evaluation\n",
        "\n",
        "* Holdout (80/20) + validação cruzada estratificada (5-fold) no treino\n",
        "\n",
        "* Compara Árvore, Bagging, AdaBoost e Random Forest usando F1 macro como métrica de seleção.\n",
        "\n",
        "* Escolhe melhor algoritmo pela média do F1 macro na CV.\n",
        "\n",
        "* Roda GridSearchCV apenas no melhor algoritmo.\n",
        "\n",
        "* Compara desempenho **antes vs. depois** do tuning no conjunto de teste e exibe matriz de confusão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "VQNB3ye2VO1U",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQNB3ye2VO1U",
        "outputId": "1bdec934-64d0-429f-c5f5-7f6b4621c8d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================== RESUMO CV (5-fold, treino) ====================\n",
            "        model  cv_acc_mean  cv_acc_std  cv_f1m_mean  cv_f1m_std\n",
            "Bagging(Tree)       0.9718      0.0119       0.9495      0.0136\n",
            " DecisionTree       0.9559      0.0113       0.9347      0.0160\n",
            " RandomForest       0.9522      0.0101       0.8830      0.0239\n",
            "     AdaBoost       0.8502      0.0170       0.4803      0.0520\n",
            "\n",
            ">> Selecionado para tuning (maior F1 macro médio): Bagging(Tree)\n",
            "\n",
            "==================== MELHOR CONJUNTO (GridSearchCV) ====================\n",
            "Algoritmo: Bagging(Tree)\n",
            "Melhor F1_macro (CV): 0.9503\n",
            "Melhores hiperparâmetros:\n",
            "  clf__estimator__max_depth: 10\n",
            "  clf__estimator__min_samples_leaf: 1\n",
            "  clf__max_features: 1.0\n",
            "  clf__max_samples: 0.8\n",
            "  clf__n_estimators: 200\n",
            "\n",
            "==================== TESTE - MODELO BASE (sem tuning) ====================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         acc     0.9733    0.9481    0.9605        77\n",
            "        good     0.8750    1.0000    0.9333        14\n",
            "       unacc     0.9917    0.9917    0.9917       242\n",
            "       vgood     1.0000    1.0000    1.0000        13\n",
            "\n",
            "    accuracy                         0.9827       346\n",
            "   macro avg     0.9600    0.9849    0.9714       346\n",
            "weighted avg     0.9832    0.9827    0.9827       346\n",
            "\n",
            "Matriz de confusão (linhas = verdade; colunas = predito):\n",
            "[[ 73   2   2   0]\n",
            " [  0  14   0   0]\n",
            " [  2   0 240   0]\n",
            " [  0   0   0  13]]\n",
            "\n",
            "==================== TESTE - MODELO TUNADO (com GridSearch) ====================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         acc     0.9474    0.9351    0.9412        77\n",
            "        good     0.9333    1.0000    0.9655        14\n",
            "       unacc     0.9876    0.9835    0.9855       242\n",
            "       vgood     0.9286    1.0000    0.9630        13\n",
            "\n",
            "    accuracy                         0.9740       346\n",
            "   macro avg     0.9492    0.9796    0.9638       346\n",
            "weighted avg     0.9742    0.9740    0.9740       346\n",
            "\n",
            "Matriz de confusão (linhas = verdade; colunas = predito):\n",
            "[[ 72   1   3   1]\n",
            " [  0  14   0   0]\n",
            " [  4   0 238   0]\n",
            " [  0   0   0  13]]\n",
            "\n",
            "==================== COMPARAÇÃO FINAL (TESTE) ====================\n",
            "               modelo acc_test f1_macro_test\n",
            " Base - Bagging(Tree)   0.9827        0.9714\n",
            "Tuned - Bagging(Tree)   0.9740        0.9638\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, GridSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ================================================================\n",
        "# 1) Dados\n",
        "# ================================================================\n",
        "car_evaluation = fetch_ucirepo(id=19)\n",
        "X = car_evaluation.data.features.copy()\n",
        "y = car_evaluation.data.targets.iloc[:, 0].copy()  # pega a primeira e única coluna de target\n",
        "\n",
        "categorical_cols = list(X.columns)\n",
        "\n",
        "# Pré-processamento: One-Hot para todas as categóricas\n",
        "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[(\"cat\", ohe, categorical_cols)],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "# ================================================================\n",
        "# 2) Split Holdout (80/20) + CV no treino\n",
        "# ================================================================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "scoring = {\"acc\": \"accuracy\", \"f1m\": \"f1_macro\"}  # usaremos f1_macro para seleção\n",
        "\n",
        "# ================================================================\n",
        "# 3) Modelos base\n",
        "#    Obs.: class_weight=\"balanced\" ajuda com desbalanceamento\n",
        "# ================================================================\n",
        "models = {\n",
        "    \"DecisionTree\": DecisionTreeClassifier(\n",
        "        criterion=\"gini\",\n",
        "        max_depth=None,\n",
        "        min_samples_leaf=1,\n",
        "        class_weight=\"balanced\",\n",
        "        random_state=RANDOM_STATE\n",
        "    ),\n",
        "    \"Bagging(Tree)\": BaggingClassifier(\n",
        "        estimator=DecisionTreeClassifier(\n",
        "            criterion=\"gini\",\n",
        "            max_depth=None,\n",
        "            min_samples_leaf=1,\n",
        "            class_weight=\"balanced\",\n",
        "            random_state=RANDOM_STATE\n",
        "        ),\n",
        "        n_estimators=200,\n",
        "        max_samples=1.0,\n",
        "        max_features=1.0,\n",
        "        bootstrap=True,\n",
        "        n_jobs=-1,\n",
        "        random_state=RANDOM_STATE\n",
        "    ),\n",
        "    \"AdaBoost\": AdaBoostClassifier(\n",
        "        # stump por default\n",
        "        n_estimators=300,\n",
        "        learning_rate=0.5,\n",
        "        random_state=RANDOM_STATE\n",
        "    ),\n",
        "    \"RandomForest\": RandomForestClassifier(\n",
        "        n_estimators=400,\n",
        "        max_depth=None,\n",
        "        min_samples_leaf=1,\n",
        "        class_weight=\"balanced\",\n",
        "        n_jobs=-1,\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "}\n",
        "\n",
        "# ================================================================\n",
        "# 4) Avaliação por CV no treino e seleção do melhor por F1 macro\n",
        "# ================================================================\n",
        "summary_rows = []\n",
        "pipes = {}\n",
        "for name, clf in models.items():\n",
        "    pipe = Pipeline(steps=[(\"prep\", preprocess), (\"clf\", clf)])\n",
        "    pipes[name] = pipe\n",
        "\n",
        "    cv_results = cross_validate(\n",
        "        pipe, X_train, y_train,\n",
        "        scoring=scoring, cv=cv, n_jobs=-1\n",
        "    )\n",
        "\n",
        "    row = {\n",
        "        \"model\": name,\n",
        "        \"cv_acc_mean\": cv_results[\"test_acc\"].mean(),\n",
        "        \"cv_acc_std\":  cv_results[\"test_acc\"].std(ddof=1),\n",
        "        \"cv_f1m_mean\": cv_results[\"test_f1m\"].mean(),\n",
        "        \"cv_f1m_std\":  cv_results[\"test_f1m\"].std(ddof=1),\n",
        "    }\n",
        "    summary_rows.append(row)\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows).sort_values(\"cv_f1m_mean\", ascending=False)\n",
        "best_model_name = summary_df.iloc[0][\"model\"]\n",
        "print(\"\\n==================== RESUMO CV (5-fold, treino) ====================\")\n",
        "print(summary_df.to_string(index=False, float_format=lambda x: f\"{x:0.4f}\"))\n",
        "print(f\"\\n>> Selecionado para tuning (maior F1 macro médio): {best_model_name}\")\n",
        "\n",
        "# ================================================================\n",
        "# 5) Tuning do melhor modelo com GridSearchCV\n",
        "#    - grades específicas para cada algoritmo\n",
        "#    - usamos F1 macro como métrica de otimização\n",
        "# ================================================================\n",
        "param_grids = {\n",
        "    \"DecisionTree\": {\n",
        "        \"clf__criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
        "        \"clf__max_depth\": [None, 5, 8, 12, 16],\n",
        "        \"clf__min_samples_leaf\": [1, 2, 4, 8, 12]\n",
        "    },\n",
        "    \"Bagging(Tree)\": {\n",
        "        \"clf__n_estimators\": [100, 200, 400],\n",
        "        \"clf__max_samples\": [0.6, 0.8, 1.0],\n",
        "        \"clf__max_features\": [0.6, 0.8, 1.0],\n",
        "        # hiper da árvore base:\n",
        "        \"clf__estimator__max_depth\": [None, 4, 6, 10],\n",
        "        \"clf__estimator__min_samples_leaf\": [1, 2, 4]\n",
        "    },\n",
        "    \"AdaBoost\": {\n",
        "        \"clf__n_estimators\": [100, 200, 300, 500],\n",
        "        \"clf__learning_rate\": [0.1, 0.3, 0.5, 1.0]\n",
        "        # (opcional) base_estimator raso:\n",
        "        # \"clf__estimator\": [DecisionTreeClassifier(max_depth=1, random_state=RANDOM_STATE)]\n",
        "    },\n",
        "    \"RandomForest\": {\n",
        "        \"clf__n_estimators\": [200, 400, 800],\n",
        "        \"clf__max_depth\": [None, 8, 12, 16],\n",
        "        \"clf__min_samples_leaf\": [1, 2, 4],\n",
        "        \"clf__max_features\": [\"sqrt\", \"log2\", None]\n",
        "    }\n",
        "}\n",
        "\n",
        "best_base_pipe = pipes[best_model_name]\n",
        "grid = GridSearchCV(\n",
        "    estimator=best_base_pipe,\n",
        "    param_grid=param_grids[best_model_name],\n",
        "    scoring=\"f1_macro\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    refit=True,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\n==================== MELHOR CONJUNTO (GridSearchCV) ====================\")\n",
        "print(f\"Algoritmo: {best_model_name}\")\n",
        "print(f\"Melhor F1_macro (CV): {grid.best_score_:0.4f}\")\n",
        "print(\"Melhores hiperparâmetros:\")\n",
        "for k, v in grid.best_params_.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# ================================================================\n",
        "# 6) Comparação em TESTE (holdout 20%): antes vs. depois do tuning\n",
        "# ================================================================\n",
        "# 6.1 modelo base (sem tuning)\n",
        "base_pipe = best_base_pipe.fit(X_train, y_train)\n",
        "y_pred_base = base_pipe.predict(X_test)\n",
        "\n",
        "print(\"\\n==================== TESTE - MODELO BASE (sem tuning) ====================\")\n",
        "print(classification_report(y_test, y_pred_base, digits=4))\n",
        "print(\"Matriz de confusão (linhas = verdade; colunas = predito):\")\n",
        "print(confusion_matrix(y_test, y_pred_base, labels=np.unique(y)))\n",
        "\n",
        "# 6.2 melhor modelo tunado\n",
        "best_tuned_pipe = grid.best_estimator_\n",
        "y_pred_tuned = best_tuned_pipe.predict(X_test)\n",
        "\n",
        "print(\"\\n==================== TESTE - MODELO TUNADO (com GridSearch) ====================\")\n",
        "print(classification_report(y_test, y_pred_tuned, digits=4))\n",
        "print(\"Matriz de confusão (linhas = verdade; colunas = predito):\")\n",
        "print(confusion_matrix(y_test, y_pred_tuned, labels=np.unique(y)))\n",
        "\n",
        "# 6.3 relatório resumido lado a lado (F1 macro e Acurácia)\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "def summarize_test(y_true, y_pred):\n",
        "    return {\n",
        "        \"acc_test\": accuracy_score(y_true, y_pred),\n",
        "        \"f1_macro_test\": f1_score(y_true, y_pred, average=\"macro\")\n",
        "    }\n",
        "\n",
        "res_base = summarize_test(y_test, y_pred_base)\n",
        "res_tuned = summarize_test(y_test, y_pred_tuned)\n",
        "\n",
        "print(\"\\n==================== COMPARAÇÃO FINAL (TESTE) ====================\")\n",
        "print(pd.DataFrame(\n",
        "    {\n",
        "        \"modelo\": [\"Base - \" + best_model_name, \"Tuned - \" + best_model_name],\n",
        "        \"acc_test\": [f\"{res_base['acc_test']:.4f}\", f\"{res_tuned['acc_test']:.4f}\"],\n",
        "        \"f1_macro_test\": [f\"{res_base['f1_macro_test']:.4f}\", f\"{res_tuned['f1_macro_test']:.4f}\"]\n",
        "    }\n",
        ").to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-Z6q70GZd4wi",
      "metadata": {
        "id": "-Z6q70GZd4wi"
      },
      "source": [
        "# 3º Pipeline -- OVO e OVR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "nrVCgHFgd96y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrVCgHFgd96y",
        "outputId": "c7278890-3501-48bd-9687-ff53e859de1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================== RESUMO CV (5-fold, treino) ====================\n",
            "         strategy  cv_acc_mean  cv_acc_std  cv_f1m_mean  cv_f1m_std\n",
            " OvO (One-vs-One)       0.9761      0.0094       0.9534      0.0339\n",
            "OvR (One-vs-Rest)       0.9501      0.0199       0.8251      0.0774\n",
            "\n",
            ">> Selecionado (maior F1 macro médio): OvO (One-vs-One)\n",
            "\n",
            "==================== TESTE — OvR (One-vs-Rest) ====================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         acc     0.9853    0.8701    0.9241        77\n",
            "        good     0.9231    0.8571    0.8889        14\n",
            "       unacc     0.9918    1.0000    0.9959       242\n",
            "       vgood     0.5714    0.9231    0.7059        13\n",
            "\n",
            "    accuracy                         0.9624       346\n",
            "   macro avg     0.8679    0.9126    0.8787       346\n",
            "weighted avg     0.9718    0.9624    0.9647       346\n",
            "\n",
            "Matriz de confusão (linhas = verdade; colunas = predito):\n",
            "[[ 67   1   2   7]\n",
            " [  0  12   0   2]\n",
            " [  0   0 242   0]\n",
            " [  1   0   0  12]]\n",
            "\n",
            "==================== TESTE — OvO (One-vs-One) ====================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         acc     0.9865    0.9481    0.9669        77\n",
            "        good     0.9286    0.9286    0.9286        14\n",
            "       unacc     0.9918    1.0000    0.9959       242\n",
            "       vgood     0.9286    1.0000    0.9630        13\n",
            "\n",
            "    accuracy                         0.9855       346\n",
            "   macro avg     0.9589    0.9692    0.9636       346\n",
            "weighted avg     0.9857    0.9855    0.9855       346\n",
            "\n",
            "Matriz de confusão (linhas = verdade; colunas = predito):\n",
            "[[ 73   1   2   1]\n",
            " [  1  13   0   0]\n",
            " [  0   0 242   0]\n",
            " [  0   0   0  13]]\n",
            "\n",
            "==================== COMPARAÇÃO FINAL (TESTE) ====================\n",
            "            model  acc_test  f1_macro_test\n",
            " OvO (One-vs-One)    0.9855         0.9636\n",
            "OvR (One-vs-Rest)    0.9624         0.8787\n",
            "\n",
            ">>> MELHOR EM TESTE (por F1 macro): OvO (One-vs-One)\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ================================================================\n",
        "# 1) Dados\n",
        "# ================================================================\n",
        "car = fetch_ucirepo(id=19)\n",
        "X = car.data.features.copy()\n",
        "y = car.data.targets.iloc[:, 0].copy()\n",
        "\n",
        "categorical_cols = list(X.columns)\n",
        "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
        "prep = ColumnTransformer([(\"cat\", ohe, categorical_cols)], remainder=\"drop\")\n",
        "\n",
        "# ================================================================\n",
        "# 2) Holdout + CV\n",
        "# ================================================================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "scoring = {\"acc\": \"accuracy\", \"f1m\": \"f1_macro\"}\n",
        "\n",
        "# ================================================================\n",
        "# 3) Classificador base (árvore) e estratégias OvR / OvO\n",
        "# ================================================================\n",
        "base_tree = DecisionTreeClassifier(\n",
        "    criterion=\"gini\",\n",
        "    max_depth=None,\n",
        "    min_samples_leaf=1,\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "ovr = OneVsRestClassifier(base_tree)\n",
        "ovo = OneVsOneClassifier(base_tree)\n",
        "\n",
        "pipelines = {\n",
        "    \"OvR (One-vs-Rest)\": Pipeline([(\"prep\", prep), (\"clf\", ovr)]),\n",
        "    \"OvO (One-vs-One)\": Pipeline([(\"prep\", prep), (\"clf\", ovo)])\n",
        "}\n",
        "\n",
        "# ================================================================\n",
        "# 4) Avaliação por CV no treino\n",
        "# ================================================================\n",
        "rows = []\n",
        "for name, pipe in pipelines.items():\n",
        "    cv_res = cross_validate(pipe, X_train, y_train, scoring=scoring, cv=cv, n_jobs=-1)\n",
        "    rows.append({\n",
        "        \"strategy\": name,\n",
        "        \"cv_acc_mean\": cv_res[\"test_acc\"].mean(),\n",
        "        \"cv_acc_std\":  cv_res[\"test_acc\"].std(ddof=1),\n",
        "        \"cv_f1m_mean\": cv_res[\"test_f1m\"].mean(),\n",
        "        \"cv_f1m_std\":  cv_res[\"test_f1m\"].std(ddof=1),\n",
        "    })\n",
        "\n",
        "cv_df = pd.DataFrame(rows).sort_values(\"cv_f1m_mean\", ascending=False)\n",
        "print(\"\\n==================== RESUMO CV (5-fold, treino) ====================\")\n",
        "print(cv_df.to_string(index=False, float_format=lambda x: f\"{x:0.4f}\"))\n",
        "\n",
        "best_name = cv_df.iloc[0][\"strategy\"]\n",
        "print(f\"\\n>> Selecionado (maior F1 macro médio): {best_name}\")\n",
        "\n",
        "# ================================================================\n",
        "# 5) Avaliação em TESTE (holdout 20%)\n",
        "# ================================================================\n",
        "def eval_on_test(name, pipe):\n",
        "    pipe.fit(X_train, y_train)\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    print(f\"\\n==================== TESTE — {name} ====================\")\n",
        "    print(classification_report(y_test, y_pred, digits=4))\n",
        "    print(\"Matriz de confusão (linhas = verdade; colunas = predito):\")\n",
        "    print(confusion_matrix(y_test, y_pred, labels=np.unique(y)))\n",
        "    return {\n",
        "        \"model\": name,\n",
        "        \"acc_test\": accuracy_score(y_test, y_pred),\n",
        "        \"f1_macro_test\": f1_score(y_test, y_pred, average=\"macro\")\n",
        "    }\n",
        "\n",
        "test_rows = []\n",
        "for name, pipe in pipelines.items():\n",
        "    test_rows.append(eval_on_test(name, pipe))\n",
        "\n",
        "test_df = pd.DataFrame(test_rows).sort_values(\"f1_macro_test\", ascending=False)\n",
        "print(\"\\n==================== COMPARAÇÃO FINAL (TESTE) ====================\")\n",
        "print(test_df.to_string(index=False, float_format=lambda x: f\"{x:0.4f}\"))\n",
        "\n",
        "winner = test_df.iloc[0][\"model\"]\n",
        "print(f\"\\n>>> MELHOR EM TESTE (por F1 macro): {winner}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85232de0",
      "metadata": {
        "id": "85232de0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "s833HXI4iYY5",
      "metadata": {
        "id": "s833HXI4iYY5"
      },
      "source": [
        "# Análise + Discussão"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-ck1alDyibcB",
      "metadata": {
        "id": "-ck1alDyibcB"
      },
      "source": [
        "**Proposta:** Comparar OvR vs OvO com Árvore de Decisão no dataset Car Evaluation\n",
        "\n",
        "Eu fiz um experimento para transformar um problema com 4 classes em vários problemas binários, usando duas estratégias diferentes:\n",
        "\n",
        "* **OvR (One-vs-Rest):** uma classe contra todas as outras.\n",
        "* **OvO (One-vs-One):** disputa entre cada par de classes.\n",
        "\n",
        "**Árvore de Decisão** como modelo base nas duas estratégias.\n",
        "\n",
        "---\n",
        "\n",
        "### Procedimento\n",
        "\n",
        "1. Separei os dados em **treino (80%)** e **teste (20%)**.\n",
        "2. No **treino**, fiz **validação cruzada com 5 partes** para ter uma avaliação mais estável.\n",
        "3. A métrica principal que escolhida foi **F1 macro**, porque ela dá o mesmo peso para cada classe, uma vez que os data set é desbalanceado. Também olhei a **acurácia** para comparar.\n",
        "\n",
        "---\n",
        "\n",
        "### Resultados\n",
        "\n",
        "**No treino (validação cruzada):**\n",
        "\n",
        "* **OvO**: F1 macro ≈ **0,953** (média)\n",
        "* **OvR**: F1 macro ≈ **0,825** (média)\n",
        "  → **OvO** foi bem melhor e também mais estável (variação menor).\n",
        "\n",
        "**No teste:**\n",
        "\n",
        "* **OvO**: F1 macro = **0,964** e acurácia = **0,986**\n",
        "* **OvR**: F1 macro = **0,879** e acurácia = **0,962**\n",
        "  → **OvO** continuou vencendo com folga.\n",
        "\n",
        "---\n",
        "\n",
        "### Análise\n",
        "\n",
        "* O **OvO** acerta melhor **todas** as classes de forma equilibrada, inclusive as **menos comuns** (como *good* e *vgood*).\n",
        "* O **OvR** teve mais dificuldade, principalmente porque “uma classe contra o resto” junta muitos casos diferentes no grupo “resto”, e a árvore tem mais chance de se confundir, gerando **falsos positivos** (por exemplo, chamando “vgood” quando não era).\n",
        "\n",
        "---\n",
        "\n",
        "### Onde o OvO se destacou\n",
        "\n",
        "* Para a classe **vgood** (que tem poucos exemplos), o **OvO** quase não errou: **lembrou de todos** (recall 100%) e quase não “inventou” vgood quando não era (alta precisão).\n",
        "* Para a classe **acc**, o **OvO** também errou menos, evitando confundir com **vgood**.\n",
        "\n",
        "---\n",
        "\n",
        "### Por que o OvO tende a ser melhor aqui\n",
        "\n",
        "* No **OvR**, o “resto” é um grupo **grande e misturado**, o que deixa a fronteira de decisão confusa.\n",
        "* No **OvO**, cada disputa é **só entre duas classes**, então a árvore consegue separar melhor, porque o problema fica **mais simples** em cada duelo.\n",
        "\n",
        "---\n",
        "\n",
        "### Conclusão\n",
        "\n",
        "* **Vencedor:** **OvO (One-vs-One) com Árvore de Decisão**.\n",
        "* **Motivo:** entregou **F1 macro maior** tanto no treino quanto no teste, mostrando que trata **todas as classes** de forma mais justa e com menos confusão.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "86169f25",
        "4vD48Mb6S-8V",
        "AnKK2G2RVL_s",
        "-Z6q70GZd4wi",
        "s833HXI4iYY5"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.13.1)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
